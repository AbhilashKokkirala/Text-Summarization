{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kaggle_data\\glove.6B.100d.txt\n",
      "./kaggle_data\\glove.6B.100d.txt.zip\n",
      "./kaggle_data\\glove.6B.200d.txt\n",
      "./kaggle_data\\glove.6B.200d.txt.zip\n",
      "./kaggle_data\\glove.6B.50d.txt\n",
      "./kaggle_data\\glove.6B.50d.txt.zip\n",
      "./kaggle_data\\news_summary.csv\n",
      "./kaggle_data\\news_summary.csv.zip\n",
      "./kaggle_data\\news_summary_more.csv\n",
      "./kaggle_data\\news_summary_more.csv.zip\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('./kaggle_data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\abhil\\OneDrive\\Documents\\AE19B017\\ML\\PLA\\Text-Summarization\\kaggle_data\\\\news_summary_more.csv'\n",
    "glove_path = 'C:\\\\Users\\\\abhil\\OneDrive\\Documents\\AE19B017\\ML\\PLA\\Text-Summarization\\kaggle_data\\glove.6B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rahat Fateh Ali Khan denies getting notice for...</td>\n",
       "      <td>Pakistani singer Rahat Fateh Ali Khan has deni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>India get all out for 92, their lowest ODI tot...</td>\n",
       "      <td>India recorded their lowest ODI total in New Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Govt directs Alok Verma to join work 1 day bef...</td>\n",
       "      <td>Weeks after ex-CBI Director Alok Verma told th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Called PM Modi 'sir' 10 times to satisfy his e...</td>\n",
       "      <td>Andhra Pradesh CM N Chandrababu Naidu has said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cong wins Ramgarh bypoll in Rajasthan, takes t...</td>\n",
       "      <td>Congress candidate Shafia Zubair won the Ramga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UP cousins fed human excreta for friendship wi...</td>\n",
       "      <td>Two minor cousins in Uttar Pradesh's Gorakhpur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>81-yr-old woman conducts physical training in ...</td>\n",
       "      <td>Isha Ghosh, an 81-year-old member of Bharat Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ram, Krishna didn't smoke, why should we: Ramd...</td>\n",
       "      <td>Urging saints and seers at the Kumbh Mela to q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pharma exec gave doctor a lap dance to sell me...</td>\n",
       "      <td>Former stripper and regional sales director of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I only cried at my 'bidaai' as I felt peer pr...</td>\n",
       "      <td>Reliance Industries' Chairman Mukesh Ambani's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Louis Vuitton owner to stockpile 4 months of w...</td>\n",
       "      <td>Louis Vuitton owner LVMH, which makes high-end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Karan Johar, Tabu turn showstoppers on opening...</td>\n",
       "      <td>Filmmaker Karan Johar and actress Tabu turned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Those on bail will go to jail: PM Modi takes j...</td>\n",
       "      <td>In a jibe at Congress President Rahul Gandhi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How long can I tolerate Congress leaders' pots...</td>\n",
       "      <td>Days after he threatened to step down from his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Odisha CM Patnaik controls mining mafia: Union...</td>\n",
       "      <td>Union Minister Dharmendra Pradhan on Wednesday...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  \\\n",
       "0   upGrad learner switches to career in ML & Al w...   \n",
       "1   Delhi techie wins free food from Swiggy for on...   \n",
       "2   New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3   Aegon life iTerm insurance plan helps customer...   \n",
       "4   Have known Hirani for yrs, what if MeToo claim...   \n",
       "5   Rahat Fateh Ali Khan denies getting notice for...   \n",
       "6   India get all out for 92, their lowest ODI tot...   \n",
       "7   Govt directs Alok Verma to join work 1 day bef...   \n",
       "8   Called PM Modi 'sir' 10 times to satisfy his e...   \n",
       "9   Cong wins Ramgarh bypoll in Rajasthan, takes t...   \n",
       "10  UP cousins fed human excreta for friendship wi...   \n",
       "11  81-yr-old woman conducts physical training in ...   \n",
       "12  Ram, Krishna didn't smoke, why should we: Ramd...   \n",
       "13  Pharma exec gave doctor a lap dance to sell me...   \n",
       "14   I only cried at my 'bidaai' as I felt peer pr...   \n",
       "15  Louis Vuitton owner to stockpile 4 months of w...   \n",
       "16  Karan Johar, Tabu turn showstoppers on opening...   \n",
       "17  Those on bail will go to jail: PM Modi takes j...   \n",
       "18  How long can I tolerate Congress leaders' pots...   \n",
       "19  Odisha CM Patnaik controls mining mafia: Union...   \n",
       "\n",
       "                                                 text  \n",
       "0   Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1   Kunal Shah's credit card bill payment platform...  \n",
       "2   New Zealand defeated India by 8 wickets in the...  \n",
       "3   With Aegon Life iTerm Insurance plan, customer...  \n",
       "4   Speaking about the sexual harassment allegatio...  \n",
       "5   Pakistani singer Rahat Fateh Ali Khan has deni...  \n",
       "6   India recorded their lowest ODI total in New Z...  \n",
       "7   Weeks after ex-CBI Director Alok Verma told th...  \n",
       "8   Andhra Pradesh CM N Chandrababu Naidu has said...  \n",
       "9   Congress candidate Shafia Zubair won the Ramga...  \n",
       "10  Two minor cousins in Uttar Pradesh's Gorakhpur...  \n",
       "11  Isha Ghosh, an 81-year-old member of Bharat Sc...  \n",
       "12  Urging saints and seers at the Kumbh Mela to q...  \n",
       "13  Former stripper and regional sales director of...  \n",
       "14  Reliance Industries' Chairman Mukesh Ambani's ...  \n",
       "15  Louis Vuitton owner LVMH, which makes high-end...  \n",
       "16  Filmmaker Karan Johar and actress Tabu turned ...  \n",
       "17  In a jibe at Congress President Rahul Gandhi, ...  \n",
       "18  Days after he threatened to step down from his...  \n",
       "19  Union Minister Dharmendra Pradhan on Wednesday...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_path,encoding='utf-8')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hi  . man tiger your caller  walle\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split()\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_mapping:\n",
    "            text[i] = contraction_mapping[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text\n",
    "\n",
    "sample = \"(hello) hi there .man tiger your's caller who's that isn't it ? WALL-E\"\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think opposition even dreams me pm modi claiming dearth ideas among opposition parties prime minister narendra modi wednesday said the opposition talks modi whole day suspect even dream me .  pm modi addressing new india youth conclave in surat added opposition parties one agenda modi . \n"
     ]
    }
   ],
   "source": [
    "data['headlines'] = data['headlines'].apply(lambda x:preprocess(x))\n",
    "data['text'] = data['text'].apply(lambda x:preprocess(x))\n",
    "print(data['headlines'][20],data['text'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former finance minister yashwant sinha tuesday demanded probe alleged diversion loans worth 31000 crore dewan housing finance  .  agencies including regulators government failed track nefarious deals said .  comes media report tuesday accused dhfl controlling shareholders diverting funds shell companies buy assets . \n",
      "yashwant sinha demands probe alleged fund diversion dhfl\n"
     ]
    }
   ],
   "source": [
    "x = data['text']\n",
    "y = data['headlines']\n",
    "print(x[50],y[50],sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Torch Req**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(summary)\n",
    "        output_lang = Lang(text)\n",
    "    else:\n",
    "        input_lang = Lang(text)\n",
    "        output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 98401 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "0        saurav kant alumnus upgrad iiitb pg program ma...\n",
      "1        kunal shah credit card bill payment platform c...\n",
      "2        new zealand defeated india 8 wickets fourth od...\n",
      "3        aegon life iterm insurance plan customers enjo...\n",
      "4        speaking sexual harassment allegations rajkuma...\n",
      "                               ...                        \n",
      "98396    crpf jawan tuesday axed death sharpedged weapo...\n",
      "98397    uff yeh first song sonakshi sinha starrer upco...\n",
      "98398    according reports new version 1999 science fic...\n",
      "98399    new music video shows rapper snoop dogg aiming...\n",
      "98400    madhesi morcha alliance seven political partie...\n",
      "Name: text, Length: 98401, dtype: object 100293\n",
      "0        upgrad learner switches career ml  al 90 salar...\n",
      "1         delhi techie wins free food swiggy one year cred\n",
      "2        new zealand end rohit sharmaled india 12match ...\n",
      "3        aegon life iterm insurance plan helps customer...\n",
      "4                 known hirani yrs metoo claims true sonam\n",
      "                               ...                        \n",
      "98396           crpf jawan axed death maoists chhattisgarh\n",
      "98397        first song sonakshi sinha noor titled uff yeh\n",
      "98398                   the matrix film get reboot reports\n",
      "98399    snoop dogg aims gun clown dressed trump new video\n",
      "98400    madhesi morcha withdraws support nepalese gove...\n",
      "Name: headlines, Length: 98401, dtype: object 41363\n",
      "['delhi igi airport deploy 1000 officers immigration counters aiming end long queues officials said .  30 increase international passengers airport last two years creating long queues immigration counters added .  authorities also plan increase number immigration counters 85 95 monthend . ', 'delhi airport get 1k immigration officers reduce queues']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData( x, y , False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print(\"Training....\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "1000 / 175001\n",
      "2000 / 175001\n",
      "3000 / 175001\n",
      "4000 / 175001\n",
      "5000 / 175001\n",
      "24m 4s (- 818m 44s) (5000 2%) 7.1685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 175000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), './enc.w')\n",
    "torch.save(attn_decoder1.state_dict(), './att.w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> mother teresa remains person till date awarded india highest civilian honour bharat ratna naturalised citizen .  mother teresa born modernday macedonia received honour january 25 1980 humanitarian work .  notably person needs resident india 12 years acquire citizenship naturalisation . \n",
      "= mother teresa naturalised indian win bharat ratna\n",
      "< us arrested made made <EOS>\n",
      "\n",
      "> kim kardashian trolled allegedly photoshopping picture car background squashed user commented thought spaceship back .  comment read just anyone else see squashed car back .  photoshopped .  caught you another user wrote kardashians fake\n",
      "= spaceship user car seen kim photoshopped pic\n",
      "< us govt made <EOS>\n",
      "\n",
      "> amazon india payments unit amazon pay reportedly talks acquire sequoiabacked allinone app tapzo 4045 million cashandstock deal .  part acquisition tapzo founders ankur singla vishal pal chaudhary likely join amazon pay team .  founded 2010 tapzo offers different services including categories like cabs food recharge bill payment . \n",
      "= amazon talks acquire sequoiabacked tapzo 40 mn\n",
      "< us arrested made made <EOS>\n",
      "\n",
      "> pictures actress katrina kaif practising stunts upcoming film tiger zinda hai surfaced online .  pictures show katrina wielding gun practising swords .  reports suggest stunt coordinators previously worked hollywood films like the dark knight rises training katrina action sequences . \n",
      "= katrina kaif practises stunts tiger zinda hai\n",
      "< us arrested made <EOS>\n",
      "\n",
      "> actor arjun kapoor featured new promo upcoming film half girlfriend asks question yeh half girlfriend kya hota hain based novel name written chetan bhagat film stars arjun character madhav jha shraddha kapoor portrays riya somani .  film release may 19 . \n",
      "= yeh half girlfriend kya hota hain asks arjun new promo\n",
      "< us arrested made made <EOS>\n",
      "\n",
      "> cement major ultratech chairman nonexecutive director kumar mangalam birla received annual remuneration 22 . 50 crore 201617 .  birla remuneration ultratech saw increase 18 . 2 previous financial year .  notably remuneration contrast 3 . 3 lakh earned group telecom arm idea cellular . \n",
      "= kumar mangalam birla took 22 . 5 crore salary ultratech\n",
      "< us modi made made <EOS>\n",
      "\n",
      "> women child development ministry proposed compulsory boys girls study home science physical education school order promote gender sensitivity .  ministry reportedly made several recommendations aimed redefining gender roles starting young age creating new employment opportunities women . \n",
      "= home science may become mandatory boys school\n",
      "< us arrested made <EOS>\n",
      "\n",
      "> maharashtra real estate regulatory authority  ordered jvpd properties private limited refund 7 . 1 crore along 15 interest 21 homebuyers mumbai .  action taken homebuyers complaint receiving letter builder stating project santacruz could completed due delay various approvals . \n",
      "= builder ordered refund 7 . 1 crore mumbai homebuyers\n",
      "< us modi made made <EOS>\n",
      "\n",
      "> west bengal hospital allegedly prescribed bottle antiseptic lotion instead cough syrup person 80 years .  since patient family could read english man found went purchase another bottle consuming first one .  concerned officer promised bear expenses man treatment . \n",
      "= bengal hospital prescribes antiseptic lotion cure cough\n",
      "< us modi made made <EOS>\n",
      "\n",
      "> philippine president rodrigo duterte ordered national police end operations war drugs 15month crackdown killed 3900 drug personalities .  philippine drug enforcement agency makes around 1 1 . 6 lakhstrong national police conduct future operations .  second time duterte suspended antidrug police operations . \n",
      "= police pulled philippine war drugs killed 3 . 9k\n",
      "< us arrested made made <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
